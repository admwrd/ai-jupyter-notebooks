{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Code a simple RAG from scratch\n",
    "\n",
    "Original article [Make Your Own RAG (Hugging Face)](https://huggingface.co/blog/ngxson/make-your-own-rag) by [Xuan-Son Nguyen](https://huggingface.co/ngxson)\n",
    "\n",
    "Recently, **Retrieval-Augmented Generation (RAG)** has emerged as a powerful paradigm in the field of AI and Large Language Models (LLMs). RAG combines information retrieval with text generation to enhance language models' performance by incorporating external knowledge sources. This approach has shown promising results in various applications, such as question answering, dialogue systems, and content generation.\n",
    "\n",
    "In this blog post, we'll explore RAG and build a simple RAG system from scratch using Python and ollama. This project will help you understand the key components of RAG systems and how they can be implemented using fundamental programming concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51220acf2df99182",
   "metadata": {},
   "source": [
    "# What is RAG\n",
    "\n",
    "To begin, let's examine a simple chatbot system **without RAG**:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    User --(1) ask--> Chatbot\n",
    "    Chatbot --(2) reply--> User\n",
    "```\n",
    "\n",
    "While the chatbot can respond to common questions based on its training dataset, it may lack access to the most up-to-date or domain-specific knowledge.\n",
    "\n",
    "A real-world example would be asking ChatGPT \"What is my mother's name?\". ChatGPT cannot answer this question because it doesn't have access to external knowledge, such as your family members' information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658c351a1ca5c8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fefefe; padding: 4rem;\">\n",
    "<div style=\"display: flex; justify-content: flex-end;\">\n",
    "  <div style=\"max-width: 50%; border-radius: 30px; background-color: #ddd; padding: 1rem; font-size: 120%; color: black;\">\n",
    "    What is my mother's name?\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"clear: both; display: flex; gap: 12px; max-width: 50%; align-items: flex-start; font-size: 120%;\">\n",
    "  <div style=\"flex-shrink: 0; width: 32px; height: 32px; border: 1px solid #ddd; border-radius: 50%; background-color: white; display: flex; align-items: center; justify-content: center; padding: 4px; box-sizing: border-box;\">\n",
    "    <img src=\"assets/openai-logo.svg\" alt=\"ChatGPT\" style=\"width: 100%; height: 100%; display: block;\">\n",
    "  </div>\n",
    "  <div style=\"flex: 1; min-width: 0; max-width: 100%; color: black;\">I don't have any way to know your mother's name, but I'm happy to help with any questions you have about names or family history!</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ec132-0b90-43d2-b77d-eefdeca8f8d1",
   "metadata": {},
   "source": [
    "To address this limitation, we need to provide external knowledge to the model (in this example, a list of family members' names):\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    User --(1) ask--> cb[Chatbot<br>&lpar;Language Model&rpar;]\n",
    "    cb --(2) query --> ks[Knowledge Source<br>&lpar;Retrieval Mode&rpar;]\n",
    "    ks --(3) knowledge --> cb\n",
    "    cb --(4) response --> User\n",
    "\n",
    "    style ks fill:#FFFF00,color:#000000\n",
    "```\n",
    "\n",
    "A RAG system consists of two key components:\n",
    "\n",
    "A **retrieval model** that fetches relevant information from an external knowledge source, which could be a database, search engine, or any other information repository.\n",
    "A **language model** that generates responses based on the retrieved knowledge.\n",
    "There are several ways to implement RAG, including Graph RAG, Hybrid RAG, and Hierarchical RAG, which we'll discuss at the end of this post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633ff85-4660-4ce2-893a-3966ed162138",
   "metadata": {},
   "source": [
    "# Simple RAG\n",
    "\n",
    "Let's create a simple RAG system that retrieves information from a predefined dataset and generates responses based on the retrieved knowledge. The system will comprise the following components:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    User --(1) ask --> em[Embedding Model]\n",
    "    em -- (2) search --> vdb[(Vector Database)]\n",
    "    vdb --(3) knowledge --> cb[Chatbot]\n",
    "    cb --(4) response --> User\n",
    "\n",
    "    style vdb fill:#FFFF00,color:#000000\n",
    "    style em fill:#FFDAB9,color:#000000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a264c-8427-426b-b301-f0f10e37e096",
   "metadata": {},
   "source": [
    "# Indexing phase\n",
    "\n",
    "The indexing phase is the first step in creating a RAG system. It involves breaking the dataset (or documents) into small **chunks** and calculating a vector representation for each chunk that can be efficiently searched during generation.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    ch1[Chunk 1] --> em[Embedding Model]\n",
    "    ch2[Chunk 2] --> em\n",
    "    ch3[Chunk 3] --> em\n",
    "    ch4[Chunk 4] --> em\n",
    "    em --> vdb[(Vector Database)]\n",
    "\n",
    "    style vdb fill:#FFFF00,color:#000000\n",
    "    style em fill:#FFDAB9,color:#000000\n",
    "```\n",
    "\n",
    "The size of each chunk can vary depending on the dataset and the application. For example, in a document retrieval system, each chunk can be a paragraph or a sentence. In a dialogue system, each chunk can be a conversation turn.\n",
    "\n",
    "After the indexing phrase, each chunk with its corresponding embedding vector will be stored in the vector database. Here is an example of how the vector database might look like after indexing:\n",
    "\n",
    "\n",
    "| Chunk Embedding                                             | Vector                            |\n",
    "|-------------------------------------------------------------|-----------------------------------|\n",
    "| Italy and France produce over 40% of all wine in the world. |  \\[0.1, 0.04, -0.34, 0.21, ...\\]  |\n",
    "| The Taj Mahal in India is made entirely out of marble.      |  \\[-0.12, 0.03, 0.9, -0.1, ...\\]  |\n",
    "| 90% of the world's fresh water is in Antarctica.            |  \\[-0.02, 0.6, -0.54, 0.03, ...\\] |\n",
    "| ...                                                         |  ...                              |\n",
    "\n",
    "The embedding vectors can be later used to retrieve relevant information based on a given query. Think of it as SQL `WHERE` clause, but instead of querying by exact text matching, we can now query a set of chunks based on their vector representations.\n",
    "\n",
    "To compare the similarity between two vectors, we can use cosine similarity, Euclidean distance, or other distance metrics. In this example, we will use cosine similarity. Here is the formula for cosine similarity between two vectors A and B:\n",
    "\n",
    "$$\n",
    "\\text{similarity}{(A,B)} = \\text{cos}(\\theta) = \\frac{{A} \\cdot {B}}{\\|{A}\\| \\|{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "\n",
    "Don't worry if you are not familiar with the formula above, we will implement it in the next section.\n",
    "\n",
    "_more info: https://audrey.feldroy.com/articles/2025-01-16-Cosine-Similarity-Breakdown-in-LaTeX_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b622d-9393-4467-b660-c07b57d063a6",
   "metadata": {},
   "source": [
    "# Retrieval phrase\n",
    "\n",
    "In the diagram below, we will take an example of a given `Input Query` from `User`. We then calculate the `Query Vector` to represent the query, and compare it against the vectors in the database to find the most relevant chunks.\n",
    "\n",
    "The result returned by The `Vector Database` will contains top N most relevant chunks to the query. These chunks will be used by the `Chatbot` to generate a response.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    User --> iq\n",
    "    subgraph query\n",
    "        direction TB\n",
    "        iq[Input Query] --> em[Embedding Model]\n",
    "        em --> qv[Query Vector]\n",
    "    end\n",
    "\n",
    "    qv --> vdb[(Vector Database)]\n",
    "    vdb --top N relevant chunks --> cb[Chatbot]\n",
    "    cb --generate response--> User\n",
    "\n",
    "    style vdb fill:#FFFF00,color:#000000\n",
    "    style em fill:#FFDAB9,color:#000000\n",
    "    style query fill:#F0F8FF,color:#000000,border:#1E90FF\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3943b-d002-4f0f-8569-be4a8b74465d",
   "metadata": {},
   "source": [
    "# Let's code it\n",
    "\n",
    "In this example, we will write a simple Python implement of RAG.\n",
    "\n",
    "To run the models, we will use ollama, a command line tool that allows you to run models from Hugging Face. With ollama, you don't need to have access to a server or cloud service to run the models. You can run the models directly on your computer.\n",
    "\n",
    "For the models, let's use the following:\n",
    "\n",
    "* **Embedding model**: [hf.co/CompendiumLabs/bge-base-en-v1.5-gguf](https://huggingface.co/CompendiumLabs/bge-base-en-v1.5-gguf)\n",
    "* **Language model**: [hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF)\n",
    "\n",
    "And for the dataset, we will use [a simple list of facts about cats](https://huggingface.co/ngxson/demo_simple_rag_py/blob/main/cat-facts.txt). Each fact will be considered as a chunk in the indexing phrase.\n",
    "\n",
    "## Download ollama and models\n",
    "\n",
    "First, let's start by installing ollama from project's website: [ollama.com](https://ollama.com).\n",
    "\n",
    "After installed, open a terminal and run the following command to download the required models:\n",
    "\n",
    "```bash\n",
    "ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "```\n",
    "\n",
    "If you see the following output, it means the models are successfully downloaded:\n",
    "\n",
    "```\n",
    "pulling manifest\n",
    "...\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "success\n",
    "```\n",
    "\n",
    "Before continuing, to use ollama in python, let's also install the ollama package:\n",
    "\n",
    "```bash\n",
    "pip install ollama\n",
    "```\n",
    "\n",
    "## Loading the dataset\n",
    "\n",
    "Next, create a Python script and load the dataset into memory. The dataset contains a list of cat facts that will be used as chunks in the indexing phrase.\n",
    "\n",
    "You can download the example dataset from [here](https://huggingface.co/ngxson/demo_simple_rag_py/blob/main/cat-facts.txt). Here is an example code to load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "source": [
    "dataset = []\n",
    "with open('data/cat-facts.txt', 'r') as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e24b618-d813-4978-81ab-525d875cee4f",
   "metadata": {},
   "source": [
    "# Implement the vector database\n",
    "\n",
    "Now, let's implement the vector database.\n",
    "\n",
    "We will use the embedding model from ollama to convert each chunk into an embedding vector, then store the chunk and its corresponding vector in a list.\n",
    "\n",
    "Here is an example function to calculate the embedding vector for a given text:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1a7df07361979dc3",
   "metadata": {},
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(current_chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=current_chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((current_chunk, embedding))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "029aa49c-4ede-4726-9a96-4cd4765b900b",
   "metadata": {},
   "source": [
    "In this example, we will consider each line in the dataset as a chunk for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "id": "688f9d7503eeb0c3",
   "metadata": {},
   "source": [
    "for i, chunk in enumerate(dataset):\n",
    "  add_chunk_to_database(chunk)\n",
    "  print(f'Added chunk {i+1}/{len(dataset)} to the database')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d63375fc-331b-4f05-a3c1-803615f773f3",
   "metadata": {},
   "source": [
    "# Implement the retrieval function\n",
    "\n",
    "Next, let's implement the retrieval function that takes a query and returns the top N most relevant chunks based on cosine similarity. We can imagine that the higher the cosine similarity between the two vectors, the \"closer\" they are in the vector space. This means they are more similar in terms of meaning.\n",
    "\n",
    "Here is an example function to calculate the cosine similarity between two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ccc2ffcadaf443ed",
   "metadata": {},
   "source": [
    "def cosine_similarity(a, b):\n",
    "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "  return dot_product / (norm_a * norm_b)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e626c94-3397-4842-acf5-d3f657d09bf2",
   "metadata": {},
   "source": [
    "Now, let's implement the retrieval function:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d6ae89415f5333f8",
   "metadata": {},
   "source": [
    "def retrieve(query, top_n=3):\n",
    "  query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "  # temporary list to store (chunk, similarity) pairs\n",
    "  similarities = []\n",
    "  for current_chunk, embedding in VECTOR_DB:\n",
    "    current_similarity = cosine_similarity(query_embedding, embedding)\n",
    "    similarities.append((current_chunk, current_similarity))\n",
    "  # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "  # finally, return the top N most relevant chunks\n",
    "  return similarities[:top_n]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4ec7cf7f-e1d4-44ec-8c27-3b89efc458ce",
   "metadata": {},
   "source": [
    "# Generation phrase\n",
    "\n",
    "In this phrase, the chatbot will generate a response based on the retrieved knowledge from the step above. This is done by simply add the chunks into the prompt that will be taken as input for the chatbot.\n",
    "\n",
    "For example, a prompt can be constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb6c727f0e4d100a",
   "metadata": {},
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "  print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "instruction_prompt = f'''You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{'\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}\n",
    "'''\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "057696da-8aa4-4084-af8b-22663b2ca8e7",
   "metadata": {},
   "source": [
    "We then use the `ollama` to generate the response. In this example, we will use `instruction_prompt` as system message:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0fc525e2a8f60ac",
   "metadata": {},
   "source": [
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "# print the response from the chatbot in real-time\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "feb4a794-c38d-48f5-b688-2112618fc629",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "You can find the final code in [this file](https://huggingface.co/ngxson/demo_simple_rag_py/blob/main/demo.py). To run the code, save it to a file named demo.py and run the following command:\n",
    "\n",
    "```bash\n",
    "python demo.py\n",
    "```\n",
    "\n",
    "You can now ask the chatbot questions, and it will generate responses based on the retrieved knowledge from the dataset.\n",
    "\n",
    "```\n",
    "Ask me a question: tell me about cat speed\n",
    "Retrieved chunks: ...\n",
    "Chatbot response:\n",
    "According to the given context, cats can travel at approximately 31 mph (49 km) over a short distance. This is their top speed.\n",
    "```\n",
    "\n",
    "# Rooms for improvement\n",
    "\n",
    "So far, we have implemented a simple RAG system using a small dataset. However, there are still many limitations:\n",
    "\n",
    "* If the question covers **multiple topics** at the same time, the system may not be able to provide a good answer. This is because the system only retrieves chunks based on the similarity of the query to the chunks, without considering the context of the query. The solution could be to have the chatbot to write its own query based on the user's input, then retrieve the knowledge based on the generated query. We can also use multiple queries to retrieve more relevant information.\n",
    "* The top N results are returned based on the cosine similarity. This may not always give the best results, especially when each chunk contains a lot of information. To address this issue, we can use a [reranking model](https://www.pinecone.io/learn/series/rag/rerankers/) to **re-rank the retrieved chunks** based on their relevance to the query.\n",
    "* The database is stored in memory, which may not be scalable for large datasets. We can use a more efficient vector database such as [Qdrant](https://qdrant.tech/), [Pinecone](https://www.pinecone.io/), [pgvector](https://github.com/pgvector/pgvector)\n",
    "* We currently consider each sentence to be a chunk. For more complex tasks, we may need to use more sophisticated techniques to **break down the dataset into smaller chunks**. We can also pre-process each chunk before adding them to the database.\n",
    "* The language model used in this example is a simple one which only has 1B parameters. For more complex tasks, we may need to use a larger language model.\n",
    "\n",
    "# Other types of RAG\n",
    "\n",
    "In practice, there are many ways to implement RAG systems. Here are some common types of RAG systems:\n",
    "\n",
    "* **Graph RAG**: In this type of RAG, the knowledge source is represented as a graph, where nodes are entities and edges are relationships between entities. The language model can traverse the graph to retrieve relevant information. There are many active researches on this type of RAG. Here is a [collection of papers on Graph RAG](https://huggingface.co/collections/graphrag/graphrag-papers-667566a057208377a1489c82).\n",
    "* **Hybrid RAG**: a type of RAG that combines Knowledge Graphs (KGs) and vector database techniques to improve question-answering systems. To know more, you can read the paper [here](https://arxiv.org/html/2408.04948v1).\n",
    "* **Modular RAG**: a type of RAG that goes beyond the basic \"retrieve-then-generate\" process, employing routing, scheduling, and fusion mechanisms to create a flexible and reconfigurable framework. This modular design allows for various RAG patterns (linear, conditional, branching, and looping), enabling more sophisticated and adaptable knowledge-intensive applications. To know more, you can read the paper [here](https://arxiv.org/html/2407.21059v1).\n",
    "For other types of RAG, you can refer to [this post by Rajeev Sharma](https://markovate.com/blog/advanced-rag-techniques/).\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "RAG represents a significant advancement in making language models more knowledgeable and accurate. By implementing a simple RAG system from scratch, we've explored the fundamental concepts of embedding, retrieval, and generation. While our implementation is basic, it demonstrates the core principles that power more sophisticated RAG systems used in production environments.\n",
    "\n",
    "The possibilities for extending and improving RAG systems are vast, from implementing more efficient vector databases to exploring advanced architectures like Graph RAG and Hybrid RAG. As the field continues to evolve, RAG remains a crucial technique for enhancing AI systems with external knowledge while maintaining their generative capabilities.\n",
    "\n",
    "\n",
    "# References\n",
    "\n",
    "* https://arxiv.org/abs/2005.11401\n",
    "* https://aws.amazon.com/what-is/retrieval-augmented-generation/\n",
    "* https://github.com/varunvasudeva1/llm-server-docs\n",
    "* https://github.com/ollama/ollama/blob/main/docs\n",
    "* https://github.com/ollama/ollama-python\n",
    "* https://www.pinecone.io/learn/series/rag/rerankers/\n",
    "* https://arxiv.org/html/2407.21059v1\n",
    "* https://newsletter.armand.so/p/comprehensive-guide-rag-implementations"
   ]
  },
  {
   "cell_type": "code",
   "id": "e05d7ed7db66b55c",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
